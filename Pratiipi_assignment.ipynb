{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "import random\n",
    "import implicit\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "user_interactions = pd.read_csv('ds-assignment/user-interactions.csv')\n",
    "meta_data = pd.read_csv('ds-assignment/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data   \n",
    "combine_data = pd.merge(user_interactions,meta_data, \n",
    "                      on ='pratilipi_id', \n",
    "                      how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting data based on publication time \n",
    "combined_data = combine_data.sort_values(\"published_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating time_spent (read_percent*reading_time)\n",
    "combine_data['time_spent'] = combine_data['read_percent']*combine_data['reading_time']/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating sparse csr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pd.DataFrame(combine_data['time_spent'].value_counts())\n",
    "_len = combine_data.shape[0]\n",
    "values = values.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values['weights'] = (values['time_spent']/_len)*20\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_percent_strength = {i: w for i,w in zip(values['index'], values['weights'])}\n",
    "read_percent_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_data['read_strength'] = combine_data['time_spent'].apply(lambda x: read_percent_strength[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_data = combine_data.drop_duplicates()\n",
    "grouped_df = combine_data.groupby(['pratilipi_id','user_id']).sum().reset_index()\n",
    "grouped_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['pratilipi_id'] = grouped_df['pratilipi_id'].astype(\"category\")\n",
    "grouped_df['user_id'] = grouped_df['user_id'].astype(\"category\")\n",
    "grouped_df['user_id_1'] = grouped_df['user_id'].cat.codes\n",
    "grouped_df['pratilipi_id_1'] = grouped_df['pratilipi_id'].cat.codes\n",
    "\n",
    "sparse_pratilipi_user = sparse.csr_matrix((grouped_df['read_strength'].astype(float), (grouped_df['pratilipi_id_1'], grouped_df['user_id_1'])))\n",
    "sparse_user_pratilipi = sparse.csr_matrix((grouped_df['read_strength'].astype(float), (grouped_df['user_id_1'], grouped_df['pratilipi_id_1'])))\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=50)\n",
    "\n",
    "alpha = 15\n",
    "data = (sparse_pratilipi_user * alpha).astype('double')\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pratilipi_id_1 = 450\n",
    "n_similar = 5\n",
    "\n",
    "user_vecs = model.user_factors\n",
    "pratilipi_vecs = model.item_factors\n",
    "\n",
    "pratilipi_norms = np.sqrt((pratilipi_vecs * pratilipi_vecs).sum(axis=1))\n",
    "\n",
    "scores = pratilipi_vecs.dot(pratilipi_vecs[pratilipi_id_1]) / pratilipi_norms\n",
    "top_idx = np.argpartition(scores, -n_similar)[-n_similar:]\n",
    "similar = sorted(zip(top_idx, scores[top_idx] / pratilipi_norms[pratilipi_id_1]), key=lambda x: -x[1])\n",
    "\n",
    "for pratilipi in similar:\n",
    "    idx, score = pratilipi\n",
    "    print(grouped_df.pratilipi_id.loc[grouped_df.pratilipi_id_1 == idx].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar books/recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id_1, sparse_user_pratilipi, pratilipi_vecs, user_vecs, num_contents=10):\n",
    "    # Get the interactions scores from the sparse person content matrix\n",
    "    person_interactions = sparse_user_pratilipi[:,user_id_1].toarray()\n",
    "    \n",
    "    # Add 1 to everything, so that articles with no interaction yet become equal to 1\n",
    "    person_interactions = person_interactions.reshape(-1) + 1\n",
    "    \n",
    "    # Make articles already interacted zero\n",
    "    person_interactions[person_interactions > 1] = 0\n",
    "    \n",
    "    # Get dot product of person vector and all content vectors\n",
    "    rec_vector = user_vecs[user_id_1,:].dot(pratilipi_vecs.T).toarray()\n",
    "    \n",
    "    # Scale this recommendation vector between 0 and 1\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    \n",
    "    # Content already interacted have their recommendation multiplied by zero\n",
    "    recommend_vector = person_interactions * rec_vector_scaled\n",
    "    \n",
    "    # Sort the indices of the content into order of best recommendations\n",
    "    content_idx = np.argsort(recommend_vector)[::-1][:num_contents]\n",
    "    \n",
    "    # Start empty list to store titles and scores\n",
    "    titles = []\n",
    "    scores = []\n",
    "\n",
    "    for idx in content_idx:\n",
    "        # Append titles and scores to the list\n",
    "        titles.append(grouped_df.pratilipi_id.loc[grouped_df.content_id == idx].iloc[0])\n",
    "        scores.append(recommend_vector[idx])\n",
    "\n",
    "    recommendations = pd.DataFrame({'title': titles, 'score': scores})\n",
    "\n",
    "    return recommendations\n",
    "    \n",
    "# Get the trained person and content vectors. We convert them to csr matrices\n",
    "person_vecs = sparse.csr_matrix(model.user_factors)\n",
    "content_vecs = sparse.csr_matrix(model.item_factors)\n",
    "\n",
    "# Create recommendations for person with id 50\n",
    "user_id_1 = 50\n",
    "\n",
    "recommendations = recommend(user_id_1=user_id_1, sparse_user_pratilipi=sparse_user_pratilipi, user_vecs=person_vecs, pratilipi_vecs=content_vecs)\n",
    "\n",
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d56d7ac4ee4883c35c50b079d388610bdf13d54aa1c1112da169cfd5d54eb276"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
